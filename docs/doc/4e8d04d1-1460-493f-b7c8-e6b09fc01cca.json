{
    "summary": "The code uses GPT-4 Vision model to evaluate image adherence to guidelines, displays results with color-coded messages after setting up test cases and formatting prompts. It also checks the result of an objective, prints outcome (PASS or FAIL) along with passed/failed tests count, and resets colors for readability.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and defining constants for the evaluation process. It appears to be setting up a test case dictionary and a function to determine if a given guideline is met in an image based on a screenshot.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/evaluate.py\":0-30",
            "content": "import sys\nimport os\nimport subprocess\nimport platform\nimport base64\nimport json\nimport openai\nfrom dotenv import load_dotenv\n# \"Objective for `operate`\" : \"Guideline for passing this test case given to GPT-4v\"\nTEST_CASES = {\n    \"Go to Github.com\": \"The Github home page is visible.\",\n    \"Go to Youtube.com and play a video\": \"The YouTube video player is visible.\",\n}\nEVALUATION_PROMPT = \"\"\"\nYour job is to look at the given screenshot and determine if the following guideline is met in the image.\nYou must respond in the following format ONLY. Do not add anything else:\n{{ \"guideline_met\": (true|false), \"reason\": \"Explanation for why guideline was or wasn't met\" }}\nguideline_met must be set to a JSON boolean. True if the image meets the given guideline.\nreason must be a string containing a justification for your decision.\nGuideline: {guideline}\n\"\"\"\nSUMMARY_SCREENSHOT_PATH = os.path.join('screenshots', 'summary_screenshot.png')\n# Check if on a windows terminal that supports ANSI escape codes\ndef supports_ansi():\n    \"\"\""
        },
        {
            "comment": "This code checks if the terminal supports ANSI escape codes and sets corresponding colors based on the platform. If supported, it defines various colored text variables. Otherwise, it sets them to empty strings. The code also includes functions for formatting an evaluation prompt and parsing evaluation content.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/evaluate.py\":31-72",
            "content": "    Check if the terminal supports ANSI escape codes\n    \"\"\"\n    plat = platform.system()\n    supported_platform = plat != \"Windows\" or \"ANSICON\" in os.environ\n    is_a_tty = hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty()\n    return supported_platform and is_a_tty\nif supports_ansi():\n    # Standard green text\n    ANSI_GREEN = \"\\033[32m\"\n    # Bright/bold green text\n    ANSI_BRIGHT_GREEN = \"\\033[92m\"\n    # Reset to default text color\n    ANSI_RESET = \"\\033[0m\"\n    # ANSI escape code for blue text\n    ANSI_BLUE = \"\\033[94m\"  # This is for bright blue\n    # Standard yellow text\n    ANSI_YELLOW = \"\\033[33m\"\n    ANSI_RED = \"\\033[31m\"\n    # Bright magenta text\n    ANSI_BRIGHT_MAGENTA = \"\\033[95m\"\nelse:\n    ANSI_GREEN = \"\"\n    ANSI_BRIGHT_GREEN = \"\"\n    ANSI_RESET = \"\"\n    ANSI_BLUE = \"\"\n    ANSI_YELLOW = \"\"\n    ANSI_RED = \"\"\n    ANSI_BRIGHT_MAGENTA = \"\"\ndef format_evaluation_prompt(guideline):\n    prompt = EVALUATION_PROMPT.format(guideline=guideline)\n    return prompt\ndef parse_eval_content(content):\n    try:\n        res = json.loads(content)"
        },
        {
            "comment": "Code function: evaluate_summary_screenshot\nPurpose: Evaluate if the summary screenshot meets a given guideline\nActions: \n1. Loads the summary screenshot\n2. Encodes it in base64 format\n3. Creates an evaluation message with text and image\n4. Sends the message to OpenAI's GPT-4 Vision model for evaluation",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/evaluate.py\":74-104",
            "content": "        print(res[\"reason\"])\n        return res[\"guideline_met\"]\n    except:\n        print(\"The model gave a bad evaluation response and it couldn't be parsed. Exiting...\")\n        exit(1)\ndef evaluate_summary_screenshot(guideline):\n    '''Load the summary screenshot and return True or False if it meets the given guideline.'''\n    with open(SUMMARY_SCREENSHOT_PATH, \"rb\") as img_file:\n        img_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n        eval_message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": format_evaluation_prompt(guideline)},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                },\n            ],\n        }]\n        response = openai.chat.completions.create(\n            model=\"gpt-4-vision-preview\",\n            messages=eval_message,\n            presence_penalty=1,\n            frequency_penalty=1,\n            temperature=0.7,\n            max_tokens=300,"
        },
        {
            "comment": "The code evaluates whether a test case meets its given guideline. It runs the \"operate\" function with the test case prompt and then calls the \"evaluate_summary_screenshot\" function to compare the result against the guideline. If the operation is successful, it prints a success message; otherwise, it prints an error message. The code loops through all the TEST_CASES, counts the number of passed and failed tests, and finally displays the results in color-coded messages.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/evaluate.py\":105-139",
            "content": "        )\n        eval_content = response.choices[0].message.content\n        return parse_eval_content(eval_content)\ndef run_test_case(objective, guideline):\n    '''Returns True if the result of the test with the given prompt meets the given guideline.'''\n    # Run `operate` with the test case prompt\n    subprocess.run(['operate', '--prompt', f'\"{objective}\"'], stdout=subprocess.DEVNULL)\n    try:\n        result = evaluate_summary_screenshot(guideline)\n    except(OSError):\n        print(\"Couldn't open the summary screenshot\")\n        return False\n    return result\ndef main():\n    load_dotenv()\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    print(f\"{ANSI_BRIGHT_MAGENTA}[STARTING EVALUATION]{ANSI_RESET}\")\n    passed = 0; failed = 0\n    for objective, guideline in TEST_CASES.items():\n        print(f\"{ANSI_BLUE}[EVALUATING]{ANSI_RESET} '{objective}'\")\n        result = run_test_case(objective, guideline)\n        if result:\n            print(f\"{ANSI_GREEN}[PASSED]{ANSI_RESET} '{objective}'\")\n            passed += 1"
        },
        {
            "comment": "The code snippet checks the result of an objective and prints the outcome (PASS or FAIL) along with the count of passed and failed tests. It resets colors for readability.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/evaluate.py\":140-149",
            "content": "        else:\n            print(f\"{ANSI_RED}[FAILED]{ANSI_RESET} '{objective}'\")\n            failed += 1\n    print(\n        f\"{ANSI_BRIGHT_MAGENTA}[EVALUATION COMPLETE]{ANSI_RESET} {passed} tests passed, {failed} tests failed\"\n    )\nif __name__ == \"__main__\":\n    main()"
        }
    ]
}