{
    "summary": "A code that utilizes AI prompts, computer vision, and OpenAI's chat completions API for generating content, including screenshots, messages, and base64 encoding images. The function captures screenshots, formats prompts, fetches asynchronous responses, extracts data, handles exceptions, and returns errors or missing labels.",
    "details": [
        {
            "comment": "Code imports various libraries and defines a function get_next_action that takes in model, messages, and objective as parameters. The code also loads a pre-trained YOLO model and initializes an OpenAI client using the configuration.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":0-50",
            "content": "import os\nimport time\nimport json\nimport base64\nimport re\nimport io\nimport asyncio\nimport aiohttp\nfrom PIL import Image\nfrom ultralytics import YOLO\nimport google.generativeai as genai\nfrom operate.settings import Config\nfrom operate.exceptions import ModelNotRecognizedException\nfrom operate.utils.screenshot import (\n    capture_screen_with_cursor,\n    add_grid_to_image,\n    capture_mini_screenshot_with_cursor,\n)\nfrom operate.utils.os import get_last_assistant_message\nfrom operate.prompts import (\n    format_vision_prompt,\n    format_accurate_mode_vision_prompt,\n    format_summary_prompt,\n    format_decision_prompt,\n    format_label_prompt,\n)\nfrom operate.utils.label import (\n    add_labels,\n    parse_click_content,\n    get_click_position_in_percent,\n    get_label_coordinates,\n)\nfrom operate.utils.style import (\n    ANSI_GREEN,\n    ANSI_RED,\n    ANSI_RESET,\n)\n# Load configuration\nconfig = Config()\nclient = config.initialize_openai_client()\nyolo_model = YOLO(\"./operate/model/weights/best.pt\")  # Load your trained model\nasync def get_next_action(model, messages, objective):"
        },
        {
            "comment": "This code checks the model parameter and calls different functions based on its value. For example, if the model is \"gpt-4\", it calls the `call_gpt_4_v` function with messages and objective parameters. It also captures a screenshot of the computer screen with the cursor.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":51-82",
            "content": "    if model == \"gpt-4\":\n        return call_gpt_4_v(messages, objective)\n    if model == \"gpt-4-with-som\":\n        return await call_gpt_4_v_labeled(messages, objective)\n    elif model == \"agent-1\":\n        return \"coming soon\"\n    elif model == \"gemini-pro-vision\":\n        return call_gemini_pro_vision(messages, objective)\n    raise ModelNotRecognizedException(model)\ndef call_gpt_4_v(messages, objective):\n    \"\"\"\n    Get the next action for Self-Operating Computer\n    \"\"\"\n    # sleep for a second\n    time.sleep(1)\n    try:\n        screenshots_dir = \"screenshots\"\n        if not os.path.exists(screenshots_dir):\n            os.makedirs(screenshots_dir)\n        screenshot_filename = os.path.join(screenshots_dir, \"screenshot.png\")\n        # Call the function to capture the screen with the cursor\n        capture_screen_with_cursor(screenshot_filename)\n        new_screenshot_filename = os.path.join(\n            \"screenshots\", \"screenshot_with_grid.png\"\n        )\n        add_grid_to_image(screenshot_filename, new_screenshot_filename, 500)"
        },
        {
            "comment": "Sleeps for 1 second, reads screenshot file, encodes image in base64, formats vision prompt with previous action, creates a vision message with the prompt and image, makes a copy of messages list, appends vision message to copied list, and then calls the OpenAI API with the updated messages list.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":83-114",
            "content": "        # sleep for a second\n        time.sleep(1)\n        with open(new_screenshot_filename, \"rb\") as img_file:\n            img_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n        previous_action = get_last_assistant_message(messages)\n        vision_prompt = format_vision_prompt(objective, previous_action)\n        vision_message = {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": vision_prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                },\n            ],\n        }\n        # create a copy of messages and save to pseudo_messages\n        pseudo_messages = messages.copy()\n        pseudo_messages.append(vision_message)\n        response = client.chat.completions.create(\n            model=\"gpt-4-vision-preview\",\n            messages=pseudo_messages,\n            presence_penalty=1,\n            frequency_penalty=1,\n            temperature=0.7,\n            max_tokens=300,"
        },
        {
            "comment": "The code is capturing a screenshot with the cursor and adding a grid overlay to the image. It then appends a message containing the filename to the messages list and returns the content of the first response choice's message. If an exception occurs during JSON parsing, it will print an error message and return a failure message.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":115-152",
            "content": "        )\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"`screenshot.png`\",\n            }\n        )\n        content = response.choices[0].message.content\n        return content\n    except Exception as e:\n        print(f\"Error parsing JSON: {e}\")\n        return \"Failed take action after looking at the screenshot\"\ndef call_gemini_pro_vision(messages, objective):\n    \"\"\"\n    Get the next action for Self-Operating Computer using Gemini Pro Vision\n    \"\"\"\n    # sleep for a second\n    time.sleep(1)\n    try:\n        screenshots_dir = \"screenshots\"\n        if not os.path.exists(screenshots_dir):\n            os.makedirs(screenshots_dir)\n        screenshot_filename = os.path.join(screenshots_dir, \"screenshot.png\")\n        # Call the function to capture the screen with the cursor\n        capture_screen_with_cursor(screenshot_filename)\n        new_screenshot_filename = os.path.join(\n            \"screenshots\", \"screenshot_with_grid.png\"\n        )\n        add_grid_to_image(screenshot_filename, new_screenshot_filename, 500)"
        },
        {
            "comment": "The code is making a computer vision model generate an action based on the screenshot, and then append the response to the messages list. If there's an exception while parsing JSON, it prints the error message and returns a failure message. The `accurate_mode_double_check` function is currently not used.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":153-188",
            "content": "        # sleep for a second\n        time.sleep(1)\n        previous_action = get_last_assistant_message(messages)\n        vision_prompt = format_vision_prompt(objective, previous_action)\n        model = genai.GenerativeModel(\"gemini-pro-vision\")\n        response = model.generate_content(\n            [vision_prompt, Image.open(new_screenshot_filename)]\n        )\n        # create a copy of messages and save to pseudo_messages\n        pseudo_messages = messages.copy()\n        pseudo_messages.append(response.text)\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"`screenshot.png`\",\n            }\n        )\n        content = response.text[1:]\n        return content\n    except Exception as e:\n        print(f\"Error parsing JSON: {e}\")\n        return \"Failed take action after looking at the screenshot\"\n# This function is not used. `-accurate` mode was removed for now until a new PR fixes it.\ndef accurate_mode_double_check(model, pseudo_messages, prev_x, prev_y):\n    \"\"\"\n"
        },
        {
            "comment": "This code takes a mini screenshot centered around the cursor and adds it to an AI prompt with text instructions. The image is encoded in base64 format and included in the prompt for further fine-tuning of clicked location.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":188-214",
            "content": "    Reprompt OAI with additional screenshot of a mini screenshot centered around the cursor for further finetuning of clicked location\n    \"\"\"\n    try:\n        screenshot_filename = os.path.join(\"screenshots\", \"screenshot_mini.png\")\n        capture_mini_screenshot_with_cursor(\n            file_path=screenshot_filename, x=prev_x, y=prev_y\n        )\n        new_screenshot_filename = os.path.join(\n            \"screenshots\", \"screenshot_mini_with_grid.png\"\n        )\n        with open(new_screenshot_filename, \"rb\") as img_file:\n            img_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n        accurate_vision_prompt = format_accurate_mode_vision_prompt(prev_x, prev_y)\n        accurate_mode_message = {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": accurate_vision_prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                },\n            ],\n        }"
        },
        {
            "comment": "Code snippet creates a prompt for the GPT-4 vision model using screenshots and text messages, then calls the \"capture_screen_with_cursor\" function.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":216-247",
            "content": "        pseudo_messages.append(accurate_mode_message)\n        response = client.chat.completions.create(\n            model=\"gpt-4-vision-preview\",\n            messages=pseudo_messages,\n            presence_penalty=1,\n            frequency_penalty=1,\n            temperature=0.7,\n            max_tokens=300,\n        )\n        content = response.choices[0].message.content\n    except Exception as e:\n        print(f\"Error reprompting model for accurate_mode: {e}\")\n        return \"ERROR\"\ndef summarize(model, messages, objective):\n    try:\n        screenshots_dir = \"screenshots\"\n        if not os.path.exists(screenshots_dir):\n            os.makedirs(screenshots_dir)\n        screenshot_filename = os.path.join(screenshots_dir, \"summary_screenshot.png\")\n        # Call the function to capture the screen with the cursor\n        capture_screen_with_cursor(screenshot_filename)\n        summary_prompt = format_summary_prompt(objective)\n        if model == \"gpt-4-vision-preview\":\n            with open(screenshot_filename, \"rb\") as img_file:"
        },
        {
            "comment": "The code is preparing input for a generative AI model. It encodes an image in base64 and combines it with a text prompt to create a summary message, then passes this message along with the chosen AI model (either gpt-4-vision-preview or gemini-pro-vision) to generate content from the summary.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":248-274",
            "content": "                img_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n            summary_message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": summary_prompt},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                    },\n                ],\n            }\n            # create a copy of messages and save to pseudo_messages\n            messages.append(summary_message)\n            response = client.chat.completions.create(\n                model=\"gpt-4-vision-preview\",\n                messages=messages,\n                max_tokens=500,\n            )\n            content = response.choices[0].message.content\n        elif model == \"gemini-pro-vision\":\n            model = genai.GenerativeModel(\"gemini-pro-vision\")\n            summary_message = model.generate_content(\n                [summary_prompt, Image.open(screenshot_filename)]\n            )"
        },
        {
            "comment": "This function calls GPT-4 with a labeled image and a prompt for decision making. It first captures a screenshot of the current desktop with the cursor, encodes it in base64 format, and adds labels to the image using the YOLO model. Then, it formats prompts for the user's decision and the GPT-4 labeling task.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":275-304",
            "content": "            content = summary_message.text\n        return content\n    except Exception as e:\n        print(f\"Error in summarize: {e}\")\n        return \"Failed to summarize the workflow\"\nasync def call_gpt_4_v_labeled(messages, objective):\n    time.sleep(1)\n    try:\n        screenshots_dir = \"screenshots\"\n        if not os.path.exists(screenshots_dir):\n            os.makedirs(screenshots_dir)\n        screenshot_filename = os.path.join(screenshots_dir, \"screenshot.png\")\n        # Call the function to capture the screen with the cursor\n        capture_screen_with_cursor(screenshot_filename)\n        with open(screenshot_filename, \"rb\") as img_file:\n            img_base64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n        previous_action = get_last_assistant_message(messages)\n        img_base64_labeled, img_base64_original, label_coordinates = add_labels(\n            img_base64, yolo_model\n        )\n        decision_prompt = format_decision_prompt(objective, previous_action)\n        labeled_click_prompt = format_label_prompt(objective)"
        },
        {
            "comment": "Creates user messages with labeled click prompt and decision prompt, appends to message lists, and fetches OpenAI response asynchronously.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":306-337",
            "content": "        click_message = {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": labeled_click_prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{img_base64_labeled}\"\n                    },\n                },\n            ],\n        }\n        decision_message = {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": decision_prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{img_base64_original}\"\n                    },\n                },\n            ],\n        }\n        click_messages = messages.copy()\n        click_messages.append(click_message)\n        decision_messages = messages.copy()\n        decision_messages.append(decision_message)\n        click_future = fetch_openai_response_async(click_messages)\n        decision_future = fetch_openai_response_async(decision_messages)"
        },
        {
            "comment": "This code fetches two responses from an API, extracts the message content, checks if it starts with \"CLICK\", gets label data and its coordinates, opens the image, retrieves its size, and calculates the click position in percent.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":339-363",
            "content": "        click_response, decision_response = await asyncio.gather(\n            click_future, decision_future\n        )\n        # Extracting the message content from the ChatCompletionMessage object\n        click_content = click_response.get(\"choices\")[0].get(\"message\").get(\"content\")\n        decision_content = (\n            decision_response.get(\"choices\")[0].get(\"message\").get(\"content\")\n        )\n        if not decision_content.startswith(\"CLICK\"):\n            return decision_content\n        label_data = parse_click_content(click_content)\n        if label_data and \"label\" in label_data:\n            coordinates = get_label_coordinates(label_data[\"label\"], label_coordinates)\n            image = Image.open(\n                io.BytesIO(base64.b64decode(img_base64))\n            )  # Load the image to get its size\n            image_size = image.size  # Get the size of the image (width, height)\n            click_position_percent = get_click_position_in_percent(\n                coordinates, image_size\n            )"
        },
        {
            "comment": "The code tries to perform a click action based on label data. If the click position percent or label is not found, it prints an error message and calls another method. It also handles exceptions and returns to try another method.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":364-386",
            "content": "            if not click_position_percent:\n                print(\n                    f\"{ANSI_GREEN}[Self-Operating Computer]{ANSI_RED}[Error] Failed to get click position in percent. Trying another method {ANSI_RESET}\"\n                )\n                return call_gpt_4_v(messages, objective)\n            x_percent = f\"{click_position_percent[0]:.2f}%\"\n            y_percent = f\"{click_position_percent[1]:.2f}%\"\n            click_action = f'CLICK {{ \"x\": \"{x_percent}\", \"y\": \"{y_percent}\", \"description\": \"{label_data[\"decision\"]}\", \"reason\": \"{label_data[\"reason\"]}\" }}'\n        else:\n            print(\n                f\"{ANSI_GREEN}[Self-Operating Computer]{ANSI_RED}[Error] No label found. Trying another method {ANSI_RESET}\"\n            )\n            return call_gpt_4_v(messages, objective)\n        return click_action\n    except Exception as e:\n        print(\n            f\"{ANSI_GREEN}[Self-Operating Computer]{ANSI_RED}[Error] Something went wrong. Trying another method {ANSI_RESET}\"\n        )\n        return call_gpt_4_v(messages, objective)"
        },
        {
            "comment": "This function makes an asynchronous API call to OpenAI's chat completions endpoint to fetch a response based on the provided messages.",
            "location": "\"/media/root/Toshiba XG3/works/self-operating-computer/docs/src/operate/actions.py\":389-408",
            "content": "async def fetch_openai_response_async(messages):\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {config.openai_api_key}\",\n    }\n    data = {\n        \"model\": \"gpt-4-vision-preview\",\n        \"messages\": messages,\n        \"frequency_penalty\": 1,\n        \"presence_penalty\": 1,\n        \"temperature\": 0.7,\n        \"max_tokens\": 300,\n    }\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            url, headers=headers, data=json.dumps(data)\n        ) as response:\n            return await response.json()"
        }
    ]
}